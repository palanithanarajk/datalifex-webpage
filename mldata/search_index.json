[
["index.html", "Data Analysis and Machine Learning Using R: Biomedial Data Preface", " Data Analysis and Machine Learning Using R: Biomedial Data Palani Thanaraj K April 2, 2019 Preface Data Science is the new buzz word in every engineering and technology field. It is used for data extraction, data analysis, data visualization and data modeling. Recently, data science is applied to biomedical field to acquire knowledge about disease occurrence and diagnosis. This introductory book provides few fundamentals about biostatistics and statistical modeling. "],
["prerequisite.html", "Prerequisite", " Prerequisite The reqirement for the readers of this book are the following: Requirement R software installed in your computer An IDE for R (Many use RStudio) Some basic knowledge about graphs and statistics "],
["introduction.html", "1 Introduction 1.1 Dataset 1.2 Data Exploration 1.3 Descriptive statistics", " 1 Introduction The center of any complex engineering application or analyis is the data/dataset. It is the root node that entire organizations and corporations run their business. There is a famous saying nowadays that Data is the new oil. In this book, we are going to focus about analyzing biomedical data. 1.1 Dataset The biomedical data we are going to use is obtained from Kaggle website. The dataset contains information about diabetes of cohort of sample subjects. This dataset arises from a research study of the National Institute of Diabetes and Digestive and Kidney Diseases. The purpose of the dataset is to predict whether or not a patient has diabetes. It is based on certain test measurements included in the dataset. Here, the patients are all females at least 21 years old of Pima Indian heritage. The datasets consists of several medical predictors/features and one target/response variable named as Outcome. We will first import the dataset into the workspace. Before that we have setup the Rstudio for carrying out the analysis. First, set the working directory using setwd(). Second, import the required library into the R Markdown file. It is shown here. setwd(&quot;C:/Users/RajuPC/Documents/MyR/RajuBook&quot;) library(tidyverse) #Required for analysis, visualization library(plotly) # Required for interactive plotting of graphs library(ggsci) #Themes package for the plots diab&lt;-read_csv(&quot;diabetes.csv&quot;) # It reads the CSV file and assigns to diab object 1.2 Data Exploration The diab is a R data frame and we can see the data in many as follows: head(diab) # Elements of first few rows ## # A tibble: 6 x 9 ## Pregnancies Glucose BloodPressure SkinThickness Insulin BMI ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 6 148 72 35 0 33.6 ## 2 1 85 66 29 0 26.6 ## 3 8 183 64 0 0 23.3 ## 4 1 89 66 23 94 28.1 ## 5 0 137 40 35 168 43.1 ## 6 5 116 74 0 0 25.6 ## # ... with 3 more variables: DiabetesPedigreeFunction &lt;dbl&gt;, Age &lt;int&gt;, ## # Outcome &lt;int&gt; tail(diab) # Elements of Last few rows ## # A tibble: 6 x 9 ## Pregnancies Glucose BloodPressure SkinThickness Insulin BMI ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 9 89 62 0 0 22.5 ## 2 10 101 76 48 180 32.9 ## 3 2 122 70 27 0 36.8 ## 4 5 121 72 23 112 26.2 ## 5 1 126 60 0 0 30.1 ## 6 1 93 70 31 0 30.4 ## # ... with 3 more variables: DiabetesPedigreeFunction &lt;dbl&gt;, Age &lt;int&gt;, ## # Outcome &lt;int&gt; colnames(diab) #Names of Columns which are the names of predictors and outcome variables ## [1] &quot;Pregnancies&quot; &quot;Glucose&quot; ## [3] &quot;BloodPressure&quot; &quot;SkinThickness&quot; ## [5] &quot;Insulin&quot; &quot;BMI&quot; ## [7] &quot;DiabetesPedigreeFunction&quot; &quot;Age&quot; ## [9] &quot;Outcome&quot; str(diab) # Structure of the dataset ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 768 obs. of 9 variables: ## $ Pregnancies : int 6 1 8 1 0 5 3 10 2 8 ... ## $ Glucose : int 148 85 183 89 137 116 78 115 197 125 ... ## $ BloodPressure : int 72 66 64 66 40 74 50 0 70 96 ... ## $ SkinThickness : int 35 29 0 23 35 0 32 0 45 0 ... ## $ Insulin : int 0 0 0 94 168 0 88 0 543 0 ... ## $ BMI : num 33.6 26.6 23.3 28.1 43.1 25.6 31 35.3 30.5 0 ... ## $ DiabetesPedigreeFunction: num 0.627 0.351 0.672 0.167 2.288 ... ## $ Age : int 50 31 32 21 33 30 26 29 53 54 ... ## $ Outcome : int 1 0 1 0 1 0 1 0 1 1 ... ## - attr(*, &quot;spec&quot;)=List of 2 ## ..$ cols :List of 9 ## .. ..$ Pregnancies : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ Glucose : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ BloodPressure : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ SkinThickness : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ Insulin : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ BMI : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; ## .. ..$ DiabetesPedigreeFunction: list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; ## .. ..$ Age : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ Outcome : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## ..$ default: list() ## .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_guess&quot; &quot;collector&quot; ## ..- attr(*, &quot;class&quot;)= chr &quot;col_spec&quot; 1.3 Descriptive statistics Here using a summary() function one can easily obtain the descriptive statistics of the imported dataframe. summary(diab) ## Pregnancies Glucose BloodPressure SkinThickness ## Min. : 0.000 Min. : 0.0 Min. : 0.00 Min. : 0.00 ## 1st Qu.: 1.000 1st Qu.: 99.0 1st Qu.: 62.00 1st Qu.: 0.00 ## Median : 3.000 Median :117.0 Median : 72.00 Median :23.00 ## Mean : 3.845 Mean :120.9 Mean : 69.11 Mean :20.54 ## 3rd Qu.: 6.000 3rd Qu.:140.2 3rd Qu.: 80.00 3rd Qu.:32.00 ## Max. :17.000 Max. :199.0 Max. :122.00 Max. :99.00 ## Insulin BMI DiabetesPedigreeFunction Age ## Min. : 0.0 Min. : 0.00 Min. :0.0780 Min. :21.00 ## 1st Qu.: 0.0 1st Qu.:27.30 1st Qu.:0.2437 1st Qu.:24.00 ## Median : 30.5 Median :32.00 Median :0.3725 Median :29.00 ## Mean : 79.8 Mean :31.99 Mean :0.4719 Mean :33.24 ## 3rd Qu.:127.2 3rd Qu.:36.60 3rd Qu.:0.6262 3rd Qu.:41.00 ## Max. :846.0 Max. :67.10 Max. :2.4200 Max. :81.00 ## Outcome ## Min. :0.000 ## 1st Qu.:0.000 ## Median :0.000 ## Mean :0.349 ## 3rd Qu.:1.000 ## Max. :1.000 Factoring the variables Factors are special type of datatype in R. It is used for categorical variables. But here the outcome variable is specified as integer. It is better to represent categorical variables as factors in R. This can be done as follows: diab$Outcome&lt;-factor(diab$Outcome) class(diab$Outcome) ## [1] &quot;factor&quot; "],
["data-visualization.html", "2 Data Visualization 2.1 Relation of diabetes and pregnancies 2.2 Relation between Glucose, Blood Pressure, Age, Pregnancy", " 2 Data Visualization Mere numbers and tables would not always provide the required information for any decision making. It is necessary that we gather some visual patterns, trends from the data to support any decision making process. This is where the data visuaization helps in getting visual perspective of the data. Basic plots such as scatter plot, bar chart, box-plot, histogram plot are great tools to provide intuitive information about the data. 2.1 Relation of diabetes and pregnancies First we try to gather the relationship between occurrence of diabetes disease and age of the subjects with the pregnancies. Note: Diabetic outcome is given as binary, where “0” refers to norm 2.1.1 Scatter plot p1&lt;-ggplot(diab,aes(x=Age,y=Pregnancies,col=Outcome))+geom_point()+geom_smooth(method=&quot;loess&quot;, se=T)+facet_grid(.~Outcome) ggplotly(p1) The above plot also shows the trend modeled through Loess method for the data provided. 2.1.2 Boxplot The plot shows the details about pregnancies and its distribution across the age of the subjects with diabetes outcome p2&lt;-ggplot(diab,aes(x=Age,y=Pregnancies))+geom_boxplot(aes(fill=Outcome))+facet_wrap(Outcome~.) ggplotly(p2) 2.1.3 Density Plot Through the density plot we can find the distribution of univariate variables in our case the pregnancies of the test subjects p3&lt;-ggplot(diab,aes(x=Pregnancies))+geom_density(aes(fill=Outcome),alpha=0.6)+ geom_vline(aes(xintercept=mean(Pregnancies)), color=&quot;blue&quot;, linetype=&quot;dashed&quot;, size=1)+facet_grid(.~Outcome)+scale_fill_aaas() ggplotly(p3) 2.2 Relation between Glucose, Blood Pressure, Age, Pregnancy 2.2.1 Scatter Plot p3&lt;-ggplot(diab,aes(x=Age, y=Pregnancies, size=Glucose, fill=BloodPressure))+geom_point(alpha=0.2)+ facet_grid(.~Outcome)+geom_jitter(width = 0.4)+scale_x_continuous(limits = c(18, 80))+scale_fill_material(&quot;red&quot;) ggplotly(p3) "],
["statistical-learning.html", "3 Statistical Learning 3.1 Data Preprocessing 3.2 Normalization of features 3.3 Options for training process 3.4 Classical ML Models 3.5 Testing the performance for the test data set.", " 3 Statistical Learning Statistical Learning is also known as Machine learning(ML) in general. Here we try to develop ML methods to model/predict the occurrence of diabetic outcome. 3.1 Data Preprocessing The data consists of different features that are needed to be mapped to a common reference frame. This is done by data preprocessing step. library(caret) # ML package for various methods # Create the training and test datasets set.seed(100) hci&lt;-diab # Step 1: Get row numbers for the training data trainRowNumbers &lt;- createDataPartition(hci$Outcome, p=0.8, list=FALSE) # Data partition for dividing the dataset into training and testing data set. This is useful for cross validation # Step 2: Create the training dataset trainData &lt;- hci[trainRowNumbers,] # Step 3: Create the test dataset testData &lt;- hci[-trainRowNumbers,] # Store X and Y for later use. x = trainData[, 1:8] y=trainData$Outcome xt= testData[, 1:8] yt=testData$Outcome # # See the structure of the new dataset 3.2 Normalization of features The features are normalized to a range of [0,1] using preproces command and using range method preProcess_range_modeltr &lt;- preProcess(trainData, method=&#39;range&#39;) preProcess_range_modelts &lt;- preProcess(testData, method=&#39;range&#39;) trainData &lt;- predict(preProcess_range_modeltr, newdata = trainData) testData &lt;- predict(preProcess_range_modelts, newdata = testData) # Append the Y variable trainData$Outcome &lt;- y testData$Outcome&lt;-yt levels(trainData$Outcome) &lt;- c(&quot;Class0&quot;, &quot;Class1&quot;) # Convert binary outcome into character for caret package levels(testData$Outcome) &lt;- c(&quot;Class0&quot;, &quot;Class1&quot;) #apply(trainData[, 1:8], 2, FUN=function(x){c(&#39;min&#39;=min(x), &#39;max&#39;=max(x))}) #str(trainData) 3.3 Options for training process #fit control fitControl &lt;- trainControl( method = &#39;cv&#39;, # k-fold cross validation number = 5, # number of folds savePredictions = &#39;final&#39;, # saves predictions for optimal tuning parameter classProbs = T, # should class probabilities be returned summaryFunction=twoClassSummary # results summary function ) 3.4 Classical ML Models The ML models we have chosen are: LDA, KNN, SVM, RandomForest, Adaboost. The Caret package provides a uniform program interface for all the machine models defined in the library. # Step 1: Tune hyper parameters by setting tuneLength set.seed(100) model1 = train(Outcome ~ ., data=trainData, method=&#39;lda&#39;, tuneLength = 5, metric=&#39;ROC&#39;, trControl = fitControl) model2 = train(Outcome ~ ., data=trainData, method=&#39;knn&#39;, tuneLength=2, trControl = fitControl)#KNN Model model3 = train(Outcome ~ ., data=trainData, method=&#39;svmRadial&#39;, tuneLength=2, trControl = fitControl)#SVM model4 = train(Outcome ~ ., data=trainData, method=&#39;rpart&#39;, tuneLength=2, trControl = fitControl)#RandomForest model5 = train(Outcome ~ ., data=trainData, method=&#39;adaboost&#39;, tuneLength=2, trControl = fitControl) # Adaboost # Compare model performances using resample() models_compare &lt;- resamples(list(LDA=model1,KNN=model2,SVM=model3,RF=model4, ADA=model5)) # Summary of the models performances summary(models_compare) ## ## Call: ## summary.resamples(object = models_compare) ## ## Models: LDA, KNN, SVM, RF, ADA ## Number of resamples: 5 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## LDA 0.8084302 0.8095930 0.8186047 0.8225000 0.8290698 0.8468023 0 ## KNN 0.7332849 0.7409884 0.7819767 0.7731395 0.7904070 0.8190407 0 ## SVM 0.7781977 0.8191860 0.8287791 0.8241860 0.8348837 0.8598837 0 ## RF 0.6415698 0.6619186 0.7068314 0.7105233 0.7531977 0.7890988 0 ## ADA 0.7741279 0.7752907 0.7796512 0.7942442 0.8098837 0.8322674 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## LDA 0.8375 0.8625 0.8750 0.8725 0.8875 0.9000 0 ## KNN 0.7750 0.7875 0.8375 0.8325 0.8625 0.9000 0 ## SVM 0.8000 0.8375 0.8500 0.8550 0.8875 0.9000 0 ## RF 0.7250 0.7625 0.7875 0.8100 0.8500 0.9250 0 ## ADA 0.7250 0.7875 0.8125 0.8075 0.8500 0.8625 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## LDA 0.4651163 0.5348837 0.5813953 0.5534884 0.5813953 0.6046512 0 ## KNN 0.3488372 0.4883721 0.4883721 0.4837209 0.5348837 0.5581395 0 ## SVM 0.4651163 0.5116279 0.5116279 0.5488372 0.5813953 0.6744186 0 ## RF 0.4186047 0.5581395 0.5813953 0.6000000 0.6511628 0.7906977 0 ## ADA 0.5581395 0.5813953 0.6279070 0.6139535 0.6279070 0.6744186 0 # Draw box plots to compare models scales &lt;- list(x=list(relation=&quot;free&quot;), y=list(relation=&quot;free&quot;)) bwplot(models_compare, scales=scales) 3.5 Testing the performance for the test data set. # Step 2: Predict on testData and Compute the confusion matrix # Using LDA Model predicted &lt;- predict(model1, testData[,1:8]) confusionMatrix(reference = testData$Outcome, data = predicted, mode=&#39;everything&#39;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction Class0 Class1 ## Class0 48 5 ## Class1 52 48 ## ## Accuracy : 0.6275 ## 95% CI : (0.5457, 0.7042) ## No Information Rate : 0.6536 ## P-Value [Acc &gt; NIR] : 0.7788 ## ## Kappa : 0.3192 ## Mcnemar&#39;s Test P-Value : 1.109e-09 ## ## Sensitivity : 0.4800 ## Specificity : 0.9057 ## Pos Pred Value : 0.9057 ## Neg Pred Value : 0.4800 ## Precision : 0.9057 ## Recall : 0.4800 ## F1 : 0.6275 ## Prevalence : 0.6536 ## Detection Rate : 0.3137 ## Detection Prevalence : 0.3464 ## Balanced Accuracy : 0.6928 ## ## &#39;Positive&#39; Class : Class0 ## "]
]
